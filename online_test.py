# TODO: Sparse Frame Inference
# TODO: Model Performance Monitoring
import cv2
import torch
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, InterpolationMode
from transformers import CLIPTextModelWithProjection, CLIPTokenizer, CLIPVisionModelWithProjection
import time

# ëª¨ë¸ ë° ì¥ì¹˜
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
text_tokenizer = CLIPTokenizer.from_pretrained("Searchium-ai/clip4clip-webvid150k")
text_model = CLIPTextModelWithProjection.from_pretrained("Searchium-ai/clip4clip-webvid150k").to(device).eval()
vision_model = CLIPVisionModelWithProjection.from_pretrained("Searchium-ai/clip4clip-webvid150k").to(device).eval()

# ì´ë¯¸ì§€ ì „ì²˜ë¦¬
transform = Compose([
    Resize(224, interpolation=InterpolationMode.BICUBIC),
    CenterCrop(224),
    lambda img: img.convert("RGB"),
    ToTensor(),
    Normalize((0.48145466, 0.4578275, 0.40821073),
              (0.26862954, 0.26130258, 0.27577711)),
])

# í…ìŠ¤íŠ¸ ì„ë² ë”©
def get_text_embedding(text):
    inputs = text_tokenizer(text, return_tensors="pt").to(device)
    with torch.no_grad():
        output = text_model(**inputs)[0]
        output = output / output.norm(dim=-1, keepdim=True)
    return output.squeeze(0)

# í”„ë ˆì„ ì„ë² ë”©
def get_frame_embedding(frame):
    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
    image_tensor = transform(image).unsqueeze(0).to(device)
    with torch.no_grad():
        output = vision_model(pixel_values=image_tensor)["image_embeds"]
        output = output / output.norm(dim=-1, keepdim=True)
    return output.squeeze(0)

# similarity ëˆ„ì ìš© ë¦¬ìŠ¤íŠ¸
similarities = []

# ì‹¤ì‹œê°„ ì¶”ë¡  ê·¸ë˜í”„ ì—…ë°ì´íŠ¸ í•¨ìˆ˜
def update_inference_plot(_): 
    global similarities, cap, text_embedding

    ret, frame = cap.read()
    if not ret:
        return

    frame_embedding = get_frame_embedding(frame)
    similarity = torch.dot(frame_embedding, text_embedding).item()
    def normalize_similarity(sim, center=0.20, sharpness=40): # í‘œë³¸ ê¸°ë°˜ ì •ê·œí™”...
        return 1 / (1 + np.exp(-sharpness * (sim - center)))
    similarity = normalize_similarity(similarity)
    similarities.append(similarity)

    # í™”ë©´ì— ìœ ì‚¬ë„ ìˆ˜ì¹˜ í‘œì‹œ
    cv2.putText(frame, f"Similarity: {similarity:.3f}", (10, 30),
                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
    cv2.imshow("Webcam Inference", frame)

    ax.clear()
    ax.plot(similarities, color="crimson")
    ax.set_title("Similarity Over Time")
    ax.set_xlabel("Frame Index")
    ax.set_ylabel("Cosine Similarity")
    ax.set_ylim(0, 1)

if __name__ == "__main__":
    # query = input("ğŸ” ë¹„êµí•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 'ì‚¬ëŒì´ ëˆ„ì›Œ ìˆë‹¤'): ")
    query = "A person is facing forward with their hand open and fingers fully extended"
    text_embedding = get_text_embedding(query)

    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("ì›¹ìº  ì—´ê¸° ì‹¤íŒ¨")
        exit()

    # ì¶”ë¡  ì‹œê°í™” ì„¤ì •
    plt.style.use('ggplot')
    fig, ax = plt.subplots(figsize=(8, 4))

    ani = FuncAnimation(fig, update_inference_plot, interval=500)  # 500ms ê°„ê²©
    plt.show()

    # ì¢…ë£Œ
    cap.release()
    cv2.destroyAllWindows()