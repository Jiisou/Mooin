import cv2
import torch
import numpy as np
from PIL import Image
from transformers import CLIPTextModelWithProjection, CLIPTokenizer, CLIPVisionModelWithProjection
from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, InterpolationMode
import time

# ì¥ì¹˜ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
text_tokenizer = CLIPTokenizer.from_pretrained("Searchium-ai/clip4clip-webvid150k")
text_model = CLIPTextModelWithProjection.from_pretrained("Searchium-ai/clip4clip-webvid150k").to(device).eval()
vision_model = CLIPVisionModelWithProjection.from_pretrained("Searchium-ai/clip4clip-webvid150k").to(device).eval()

# ì „ì²˜ë¦¬ í•¨ìˆ˜
transform = Compose([
    Resize(224, interpolation=InterpolationMode.BICUBIC),
    CenterCrop(224),
    lambda img: img.convert("RGB"),
    ToTensor(),
    Normalize((0.48145466, 0.4578275, 0.40821073),
              (0.26862954, 0.26130258, 0.27577711)),
])

def get_text_embedding(text):
    inputs = text_tokenizer(text, return_tensors="pt").to(device)
    with torch.no_grad():
        output = text_model(**inputs)[0]
        output = output / output.norm(dim=-1, keepdim=True)
    return output.squeeze(0)

def get_frame_embedding(frame):
    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
    image_tensor = transform(image).unsqueeze(0).to(device)
    with torch.no_grad():
        output = vision_model(pixel_values=image_tensor)["image_embeds"]
        output = output / output.norm(dim=-1, keepdim=True)
    return output.squeeze(0)

# Real-time inference loop
def run_online_inference(text_prompt):
    text_embedding = get_text_embedding(text_prompt)

    cap = cv2.VideoCapture(0)  # ê¸°ë³¸ ì¹´ë©”ë¼
    if not cap.isOpened():
        print("ì›¹ìº  ì—´ê¸°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return

    print("ğŸ” ì‹¤ì‹œê°„ ì¶”ë¡ ì„ ì‹œì‘í•©ë‹ˆë‹¤. 'q'ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œë©ë‹ˆë‹¤.")
    while True:
        ret, frame = cap.read()
        if not ret:
            print("í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            break

        # ì¶”ë¡ 
        frame_embedding = get_frame_embedding(frame)
        similarity = torch.dot(frame_embedding, text_embedding).item()

        # í™”ë©´ì— ìœ ì‚¬ë„ í‘œì‹œ
        cv2.putText(frame, f"Similarity: {similarity:.3f}", (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        cv2.imshow("VLM Real-Time Inference", frame)

        # ì¢…ë£Œ ì¡°ê±´
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    query = input("ğŸ” ë¹„êµí•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 'ì‚¬ëŒì´ ëˆ„ì›Œ ìˆë‹¤'): ")
    run_online_inference(query)
