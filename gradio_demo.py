import gradio as gr
import torch
import numpy as np
import cv2
from PIL import Image
from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, InterpolationMode
from transformers import (
    CLIPTextModelWithProjection,
    CLIPTokenizer,
    CLIPVisionModelWithProjection
)
import matplotlib.pyplot as plt

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”©
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
text_tokenizer = CLIPTokenizer.from_pretrained("Searchium-ai/clip4clip-webvid150k")
text_model = CLIPTextModelWithProjection.from_pretrained("Searchium-ai/clip4clip-webvid150k").to(device).eval()
vision_model = CLIPVisionModelWithProjection.from_pretrained("Searchium-ai/clip4clip-webvid150k").to(device).eval()

# ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess(size, image):
    transform = Compose([
        Resize(size, interpolation=InterpolationMode.BICUBIC),
        CenterCrop(size),
        lambda img: img.convert("RGB"),
        ToTensor(),
        Normalize((0.48145466, 0.4578275, 0.40821073),
                  (0.26862954, 0.26130258, 0.27577711)),
    ])
    return transform(image)

# ì˜ìƒ â†’ í”„ë ˆì„ ì´ë¯¸ì§€ í…ì„œ ë¦¬ìŠ¤íŠ¸
def video_to_frames(video_path, frame_rate=1, size=224):
    cap = cv2.VideoCapture(video_path)
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    frames = []
    count = 0
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        if count % int(fps / frame_rate) == 0:
            img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            tensor = preprocess(size, img)
            frames.append(tensor)
        count += 1
    cap.release()
    return torch.stack(frames).to(device)

# í…ìŠ¤íŠ¸ ì„ë² ë”© ì¶”ì¶œ
def get_text_embedding(text):
    inputs = text_tokenizer(text, return_tensors="pt").to(device)
    with torch.no_grad():
        output = text_model(**inputs)[0]
        output = output / output.norm(dim=-1, keepdim=True)
    return output

# ì˜ìƒ ì„ë² ë”© ì¶”ì¶œ (í”„ë ˆì„ ë‹¨ìœ„)
def get_video_embeddings(frames):
    with torch.no_grad():
        outputs = vision_model(pixel_values=frames)["image_embeds"]
        outputs = outputs / outputs.norm(dim=-1, keepdim=True)
    return outputs

# ìœ ì‚¬ë„ ê³„ì‚° ê²°ê³¼ ë°˜í™˜
def analyze(video, text):
    # frames = video_to_frames(video.name) ;gr.Video ì»´í¬ë„ŒíŠ¸ëŠ” ê¸°ë³¸ê°’(type="filepath")ìœ¼ë¡œ ë‹¨ìˆœ ë¬¸ìì—´ ê²½ë¡œë¥¼ ë°˜í™˜
    frames = video_to_frames(video)
    video_embeddings = get_video_embeddings(frames)
    text_embedding = get_text_embedding(text)

    # ìœ ì‚¬ë„ ê³„ì‚°
    text_vector = text_embedding.squeeze(0)
    sims = torch.matmul(video_embeddings, text_vector).cpu().numpy()

    # ê·¸ë˜í”„ ê°ì²´ ìƒì„±í•´ ì‹œê°í™”
    fig, ax = plt.subplots(figsize=(12, 3))
    ax.plot(sims, color="crimson", linewidth=2)
    ax.set_title(f"Frame-wise Cosine Similarity to: \"{text}\"")
    ax.set_xlabel("Frame Index")
    ax.set_ylabel("Similarity")
    ax.grid(True)

    return float(np.mean(sims)), float(np.max(sims)), fig

# Gradio ì¸í„°í˜ì´ìŠ¤ êµ¬ì„±
demo = gr.Interface(

    fn=analyze,
    inputs=[
        gr.Video(label="ì˜ìƒ ë¶ˆëŸ¬ì˜¤ê¸°"),
        gr.Textbox(label="íƒì§€ ìƒí™© ì„¤ëª… ì…ë ¥", placeholder="ì˜ˆ: 'ì‚¬ëŒì´ ëˆ„ì›Œìˆë‹¤'")
    ],
    outputs=[
        gr.Number(label="ğŸ“ˆ í‰ê·  ìœ ì‚¬ë„"),
        gr.Number(label="ğŸ“ ìµœëŒ€ ìœ ì‚¬ë„"),
        gr.Plot(label="ğŸ“Š í”„ë ˆì„ë³„ ìœ ì‚¬ë„ ê·¸ë˜í”„")
    ],
    title="VLM Textâ€“Video ìœ ì‚¬ë„ ë¶„ì„ ë°ëª¨ ğŸï¸",
    description="ì…ë ¥í•œ í…ìŠ¤íŠ¸ì™€ ì˜ìƒ í”„ë ˆì„ ê°„ ìœ ì‚¬ë„ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.",
).launch(share=True)  # share=True ì¶”ê°€í•˜ë©´ ê³µê°œ ë§í¬ ìƒì„±ë¨


demo.launch()